%!TEX root = ../main.tex

\chapter{基于特征融合的红外小目标检测方法}


\section{本章概述}

红外小目标检测（IRSTD）作为计算机视觉和遥感领域中的一个重要问题，广泛应用于军事监视、航空航天监测、海上搜救等多个领域。然而，由于红外图像中小目标与复杂背景之间的对比度低、尺寸小以及可能受到噪声和遮挡的影响，红外小目标的检测任务面临着极大的挑战。尤其是当红外目标与背景的对比度较低时，传统的目标检测方法往往难以准确分辨目标信息，导致检测性能显著下降。

传统的红外小目标检测方法主要依赖于背景建模、局部对比度增强以及低秩约束等技术。这些方法在抑制背景噪声、增强目标信息方面有一定的效果，但它们通常需要大量的先验知识，且在复杂背景下往往会出现较高的误报率（False Alarm，Fa）和漏检率（Miss Detection）。随着深度学习技术的快速发展，尤其是卷积神经网络（CNN）和U-Net等架构在图像分割和目标检测领域的成功应用，数据驱动的深度学习方法成为解决红外小目标检测问题的主流。

U-Net网络由于其独特的编码器-解码器结构以及跳跃连接，已经被广泛应用于红外小目标检测任务。通过编码器提取图像的高层次语义特征，并通过解码器恢复空间分辨率，U-Net能够有效保留低层次的细节信息，从而提高检测精度。然而，传统的U-Net在进行特征融合时，低层的空间细节信息与高层的语义信息往往存在不匹配的问题，导致在深层网络中目标特征的丢失和目标轮廓的模糊。这一问题限制了U-Net在红外小目标检测中的应用，尤其是在复杂背景和目标轮廓较为模糊的场景中，检测效果不尽如人意。

为了克服这些问题，本章提出了一种基于双向特征融合（Bidirectional Feature Fusion，BDF）的改进型U-Net架构——BDFU-Net。该方法通过引入双向特征融合模块（BDF），优化了传统U-Net架构中的跳跃连接，使得模型在进行特征融合时，能够有效传递浅层的空间细节信息和深层的语义信息。具体来说，本方法设计了两种特征融合方式：首先是下行特征融合模块（DFF），通过将浅层特征向深层传递，有效去除背景噪声并增强目标特征；其次是上行特征融合模块（UFF），通过将深层的语义信息反馈到浅层，进一步细化目标的轮廓和位置。双向特征融合策略使得BDFU-Net能够更好地弥合浅层和深层特征之间的差异，增强模型对目标细节的捕捉能力，并在复杂背景下有效提高目标检测的鲁棒性。

此外，本章还在BDFU-Net的基础上引入了深监督机制，通过在网络的不同层级进行多尺度预测，进一步提升了网络的训练效果和收敛速度。深监督机制使得模型能够在不同尺度下进行特征学习，从而提高对小目标的检测精度和细节恢复能力。

本章的结构安排如下：首先，我们回顾了U-Net的基本架构及其在红外小目标检测中的应用，分析了传统方法的局限性。接着，详细介绍了BDFU-Net的网络结构，包括双向特征融合模块的设计理念、下行与上行特征融合的实现方法以及深监督机制的应用。最后，我们将讨论如何通过这些创新性的设计来提高红外小目标检测的准确性和鲁棒性，特别是在复杂背景下，如何更好地区分目标与背景，并恢复更多的目标细节信息。

通过这些创新，BDFU-Net能够有效解决传统U-Net在红外小目标检测中的不足，尤其是在面对复杂背景和低对比度目标时，能够实现更为精确的目标检测。


\section{UNEt的使用}

\subsection{U-Net架构及传统跳跃连接的局限性}
U-Net架构最初由Ronneberger等人于2015年提出，旨在解决医学图像分割中的挑战。由于其优异的性能和高度的灵活性，U-Net在各种图像分割任务中得到了广泛应用，尤其是在需要精确边缘定位和细节恢复的任务中。红外小目标检测（IRSTD）作为一种典型的目标检测任务，采用U-Net作为基础架构，能够有效地从复杂背景中提取目标信息并准确地进行分割。

\subsubsection{编码器——解码器结构}
U-Net采用了典型的编码器-解码器（Encoder-Decoder）结构。编码器部分通过一系列卷积和池化操作逐步提取图像的高层次语义特征。每一层的卷积操作在空间维度上逐渐降低图像的分辨率，同时增加特征图的通道数，从而有效地压缩图像信息并捕捉到更为抽象的特征。池化操作通常采用最大池化（Max Pooling）方式，进一步减小特征图的尺寸，并增强网络的感受野。

解码器部分则通过上采样操作逐步恢复图像的空间分辨率，使得模型能够生成与原图像相同大小的预测结果。通过反卷积或上采样操作，解码器在恢复空间信息的同时，也能够保留更多的细节特征，从而实现精细的目标分割。

\subsubsection{跳跃连接的作用}
U-Net的一个关键创新是引入了跳跃连接（Skip Connection）机制。传统的编码器-解码器结构在降维过程中会丧失大量的细节信息，尤其是在图像的低层次特征中，这对目标检测尤其是小目标检测非常不利。U-Net通过在编码器和解码器之间添加跳跃连接，直接将编码器中低层次的特征图传递给解码器的对应层，从而避免了细节信息的丢失。

跳跃连接不仅能够增强低层次特征的传递，还能够帮助解码器更好地恢复图像细节。在红外小目标检测任务中，跳跃连接使得U-Net能够在保持高层语义信息的同时，恢复重要的空间细节，从而提升了小目标的检测精度。

\subsubsection{传统skip-connection的局限性}
尽管U-Net通过跳跃连接有效地结合了高层语义特征和低层空间特征，但传统的跳跃连接仍然存在一定的局限性。在红外小目标检测任务中，由于目标的尺度较小且与背景的对比度较低，低层次的细节信息与高层次的语义信息在融合过程中可能出现不匹配的问题。这种信息的不对齐可能导致目标轮廓模糊，甚至在复杂背景下发生目标信息丢失。

例如，低层特征通常包含丰富的空间细节信息，但它们可能包含较多的背景噪声，而高层特征则提供了较为明确的目标语义信息，但却缺乏足够的空间分辨率。因此，传统的U-Net在进行特征融合时，无法有效地平衡低层次空间细节和高层次语义信息之间的差异，导致目标的精细轮廓无法准确恢复。



\section{BDFunet的动机}
在红外小目标检测（Infrared Small Target Detection, IRSTD）任务中，由于目标的尺寸小、对比度低以及复杂背景的影响，传统的检测方法往往面临着目标信息丢失和目标轮廓模糊的问题。尤其是在深层网络中，目标特征可能会被背景噪声所掩盖，从而导致检测精度的下降。因此，如何有效融合低层次的空间细节和高层次的语义信息，并且在保持目标细节的同时抑制背景噪声，成为了提升红外小目标检测性能的关键问题。

为了克服这一挑战，本论文提出的BDFU-Net（Bidirectional Feature Fusion U-Net）架构中，设计了双向特征融合模块（Bidirectional Feature Fusion, BDF），旨在通过增强浅层与深层特征之间的联系来解决信息丢失的问题。BDF模块的设计思想主要体现在以下方面：

首先，针对深层语义信息与浅层空间细节的不匹配问题，在传统的U-Net架构中，浅层特征图能够捕捉到图像的细节信息（如目标的轮廓、纹理和位置等），但这些特征往往包含大量的背景噪声；而深层特征图则提供了丰富的语义信息，能够准确地表达目标的类别和语义意义，但由于下采样操作的影响，空间分辨率较低，细节信息容易丢失。因此，浅层和深层特征在信息表达上存在较大的差异，其有效融合是提升检测精度的关键。BDF模块通过设计双向信息传递策略，即从浅层到深层的下行融合（Downward Feature Fusion, DFF）和从深层到浅层的上行融合（Upward Feature Fusion, UFF）两轮特征融合，旨在弥合浅层空间细节和深层语义信息之间的差距。通过这种双向融合，BDF模块能够确保低层的空间信息得到保留，同时加强高层的语义表达，从而提高目标的检测能力。

其次，为保持目标轮廓的清晰度并抑制背景噪声，红外图像中的小目标通常与背景之间的对比度较低，且目标形状可能不规则，因此准确检测目标轮廓至关重要。传统的特征融合方法往往会忽略这一点，在将浅层与深层特征进行简单融合时，可能会导致目标轮廓的模糊或丢失。为解决这一问题，BDF模块引入了空间注意力机制（Spatial Attention），通过对低层特征进行空间加权，帮助突出目标区域的细节信息，并有效抑制背景噪声。尤其是在下行融合（DFF）阶段，空间注意力帮助去除背景噪声，保留与目标相关的空间细节；在上行融合（UFF）阶段，空间注意力则增强目标轮廓的恢复，使得目标的细节得以清晰还原。

最后，双向特征融合策略能够提升目标检测的鲁棒性与边缘清晰度。在复杂场景中，背景的变化和噪声可能严重干扰目标的识别，特别是当目标与背景的对比度较低时，传统的方法往往会产生较多的误报和漏检。通过双向特征融合，BDF模块可以在保留背景信息的同时，强调目标区域的细节，确保目标特征在深层和浅层特征之间得到充分的传递与强化。这种增强的信息流动提升了模型在各种复杂背景下的鲁棒性，使得BDFU-Net能够更好地区分目标与背景，并恢复目标边缘的清晰度，从而减少误报率和漏检率。







\section{BDFU-Net 总体流程}


BDFU-Net（Bidirectional Feature Fusion U-Net）是一种改进型的U-Net架构，旨在解决红外小目标检测中的关键问题，尤其是目标信息丢失和目标轮廓模糊的问题。BDFU-Net通过引入双向特征融合模块（BDF）以及深监督机制，优化了传统U-Net在处理低对比度小目标时的性能，能够在复杂背景下更有效地分离目标和背景，恢复目标细节。



% htbp 什么的现在不要管
\begin{figure}[htbp]
    \centering  % 学位论文规定图表皆水平居中于版心 在 zjuthesis.cls 搜「版心设置」
    \includegraphics[width = \linewidth]{overall.png} % 设定图片宽度相对于版心宽度，图片文件资源名
    \caption{华莱士在其著作《马来群岛》中绘制的飞蛙速写} % 图的题注
    \label{fig:plus-1} % 与 autoref 关联，设定交叉引用和显示「图x.x」
\end{figure}



BDFU-Net的总体流程包括四个主要步骤：特征提取、双向特征融合、特征融合与重建、以及最终输出预测。

1. 特征提取

BDFU-Net的输入是一个红外图像，网络首先通过一系列的卷积层和池化操作进行特征提取。网络采用了四级编码器（Encoder）结构，在每一级中通过卷积操作提取不同层次的特征，同时通过最大池化（Max Pooling）操作逐渐减少特征图的空间尺寸，增加特征图的通道数。这一过程能够逐步捕捉图像的高层语义信息。

2. 双向特征融合

在特征提取后，BDFU-Net引入了双向特征融合模块（BDF），该模块通过两轮融合策略有效地处理了低层次的空间细节信息与高层次的语义信息之间的差异。具体而言，网络首先通过\textbf{下行特征融合模块（DFF）}将浅层特征传递到深层，帮助去除背景噪声并保留目标特征。然后，网络通过\textbf{上行特征融合模块（UFF）}将深层语义信息反馈到浅层，以增强目标轮廓的细节并恢复目标位置。

3. 特征融合与重建

双向特征融合之后，BDFU-Net使用\textbf{Channel-wise Cross Attention（CCA）}机制进一步融合高层与低层特征。CCA机制通过关注特征图中的通道信息，进一步强化不同层次之间的特征交互，优化特征的表示能力。融合后的特征图通过解码器逐步恢复空间分辨率，最终生成与输入图像相同尺寸的输出预测。

4. 最终输出预测

在网络的最后，BDFU-Net通过深监督机制进行多尺度预测。每一层解码器的输出都会进行上采样和融合，以生成最终的目标检测结果。具体来说，网络会输出目标的掩膜图（Mask），用于表示图像中各个区域是否包含目标。通过深监督的方式，网络在训练过程中能够有效地利用多尺度信息进行优化，从而提高检测精度。

输入与输出示例

以一个256$\times$256大小的红外图像为输入，经过BDFU-Net的特征提取和双向特征融合后，网络输出的预测结果也是256$\times$256的图像。输出图像中，目标区域会被精确分割，背景被有效抑制。假设输入图像中包含一个小目标，BDFU-Net能够通过其双向特征融合机制有效地保留目标细节并清晰地描绘目标的轮廓，从而实现精确的目标检测。



\section{具体模块设计设计}


\subsection{下行特征融合模块（DFF）}

下行特征融合模块（Down Feature Fusion, DFF）是BDFU-Net中的关键模块之一，旨在通过将浅层特征信息传递到深层，去除背景噪声并保留目标特征。红外小目标通常具有较小的尺度和较低的对比度，低层特征图包含丰富的空间细节信息，但也往往伴随着较多的背景噪声。DFF模块通过对浅层和深层特征的融合，增强目标信息的表达，并有效地抑制无关背景的干扰。



% htbp 什么的现在不要管
\begin{figure}[htbp]
    \centering  % 学位论文规定图表皆水平居中于版心 在 zjuthesis.cls 搜「版心设置」
    \includegraphics[width = \linewidth]{DFF.png} % 设定图片宽度相对于版心宽度，图片文件资源名
    \caption{华莱士在其著作《马来群岛》中绘制的飞蛙速写} % 图的题注
    \label{fig:plus-1} % 与 autoref 关联，设定交叉引用和显示「图x.x」
\end{figure}



\subparagraph{模块细节设计}

在DFF模块中，首先将相邻两层的特征图 $E_i$ 和 $E_{i+1}$ 输入到模块中，其中 $E_i$ 表示浅层特征图，$E_{i+1}$ 表示深层特征图。为了确保融合过程中的信息一致性，DFF模块采用了一种自定义的下采样操作。该操作通过像素重排（Pixel Reorganization）将 $E_i$ 转换为与 $E_{i+1}$ 相同的空间尺寸，从而使得两个特征图可以进行有效融合。

公式如下所示：

\begin{equation}
E_i' = \sigma(\text{conv}(\text{avg}(E_{i+1}))) \odot \text{Down}(E_i)
\end{equation}

其中，$\sigma$ 表示Sigmoid激活函数，$\text{conv}$ 表示卷积操作，$\text{avg}$ 是全局平均池化操作，$\odot$ 表示逐元素相乘，$\text{Down}(E_i)$ 表示通过像素重排将 $E_i$ 转换为 $E_{i+1}$ 大小的过程。

通过这种方式，DFF模块能够生成加权后的浅层特征 $E_i'$，并且将其与深层特征 $E_{i+1}$ 进行融合，生成新的特征图 $E_{i+1}'$：

\begin{equation}
E_{i+1}' = \sigma(\text{conv}(E_i')) \odot E_{i+1}
\end{equation}

最后，两个经过处理的特征图 $E_i'$ 和 $E_{i+1}'$ 被相加并通过1x1卷积进行融合，得到最终的下行融合特征：

\begin{equation}
E_{i+1}'' = \text{conv}(E_i' + E_{i+1}')
\end{equation}

这种融合方式有效地将低层的空间信息传递到深层，同时利用全局平均池化和注意力机制抑制了背景噪声，增强了目标信息的表现。

\subsection{上行特征融合模块（UFF）}

上行特征融合模块（Up Feature Fusion, UFF）是BDFU-Net中的另一重要模块，旨在通过将深层的语义信息传递到浅层，进一步增强目标细节和轮廓的恢复。在红外小目标检测中，浅层特征包含了丰富的空间信息，但往往缺乏准确的语义表示，而深层特征则提供了更为精确的目标语义信息，但其空间分辨率较低。UFF模块通过将深层特征和浅层特征相结合，能够有效地恢复目标的边缘和细节。




% htbp 什么的现在不要管
\begin{figure}[htbp]
    \centering  % 学位论文规定图表皆水平居中于版心 在 zjuthesis.cls 搜「版心设置」
    \includegraphics[width = \linewidth]{UFF.png} % 设定图片宽度相对于版心宽度，图片文件资源名
    \caption{华莱士在其著作《马来群岛》中绘制的飞蛙速写} % 图的题注
    \label{fig:plus-1} % 与 autoref 关联，设定交叉引用和显示「图x.x」
\end{figure}




\subparagraph{模块细节设计}
在UFF模块中，浅层特征图 $E_i$ 和上采样后的深层特征图 $E_{i+1}$ 作为输入，首先进行空间注意力计算。通过卷积操作和Sigmoid激活函数，生成对应的空间权重 $w_i$ 和 $w_{i+1}$，它们分别表示浅层和深层特征图中各像素点属于目标的概率。公式如下：

\begin{equation}
w_i = \sigma(\text{conv}(E_i))
\end{equation}

\begin{equation}
w_{i+1} = \sigma(\text{conv}(\text{Up}(E_{i+1})))
\end{equation}

其中，$\text{Up}(E_{i+1})$ 表示对深层特征图 $E_{i+1}$ 进行上采样操作。

接着，空间权重 $w_i$ 和 $w_{i+1}$ 被沿着通道维度拼接，得到融合的空间权重 $w_{\text{fusion}}$，公式如下：

\begin{equation}
w_{\text{fusion}} = \text{concat}(w_i, w_{i+1})
\end{equation}

然后，$w_{\text{fusion}}$ 经过卷积操作压缩成一个单通道，得到最终的融合空间权重：

\begin{equation}
w_{\text{fusion}} = \text{conv}(w_{\text{fusion}})
\end{equation}

接下来，将上采样后的 $E_{i+1}$ 和浅层特征图 $E_i$ 进行拼接，并与空间权重 $w_{\text{fusion}}$ 进行逐元素相乘，从而实现目标细节的恢复。公式如下：

\begin{equation}
O_i' = w_{\text{fusion}} \odot \text{concat}(E_i, \text{Up}(E_{i+1}))
\end{equation}

最后，经过 ECA（Efficient Channel Attention）模块的进一步处理，最终输出融合后的特征图 $O_i'$，该特征图包含了来自深层和浅层的信息，能够有效地提升目标的边缘清晰度和位置精度。




\section{损失函数设计与深监督机制}

在红外小目标检测任务中，由于目标尺寸较小、背景复杂以及目标与背景之间的对比度低，传统的损失函数（如交叉熵损失）往往难以有效地处理这些挑战。因此，设计一个能够更好地平衡小目标检测精度与背景抑制的损失函数是提高模型性能的关键。本节将介绍BDFU-Net中使用的损失函数设计和深监督机制，旨在通过多尺度损失与深监督的结合，提升红外小目标检测的鲁棒性与精度。

\subsection{深监督结构}

深监督机制（Deep Supervision）是一种通过在网络的不同层次引入额外的损失函数进行训练的方法。其主要思想是通过多尺度的损失来引导网络学习更丰富的特征，从而加速网络的收敛并提高检测精度。在BDFU-Net中，我们在解码器的多个层次输出中引入深监督，通过这些输出的损失对网络进行优化。

在BDFU-Net中，解码器的每一层都会输出一组多尺度特征图 $F_1, F_2, F_3, F_4$，其中每个特征图都通过1×1卷积进行处理，并经过Sigmoid激活函数得到对应的显著性图 $M_i$（即目标与背景的分割结果）。这些显著性图表示图像中目标区域的概率分布。

因此，深监督结构的目标是通过对每个尺度输出计算损失函数来优化模型。具体而言，每个输出特征图 $F_i$ 经过1×1卷积后生成一个显著性图 $M_i$，然后通过与真实标签 $Y$（ground truth）计算二值交叉熵（Binary Cross-Entropy, BCE）损失来衡量预测结果与真实标签之间的差异。

损失公式如下：

\begin{equation}
M_i = \sigma(f_{1\times1}(F_i)) \quad (i = 1, 2, 3, 4)
\end{equation}

其中，$\sigma$ 表示Sigmoid函数，$f_{1\times1}$ 是1×1卷积操作，$F_i$ 是解码器第$i$层输出的特征图。

\subsection{损失函数框架}

为了使模型更好地学习目标的细节信息，BDFU-Net结合了多个尺度的损失函数。每个尺度的损失通过计算二值交叉熵损失函数来进行评估，并通过深监督机制加权合并，以确保每一层的输出都能够在训练过程中对网络产生正向的影响。

对于每个显著性图 $M_i$，我们计算其与真实标签 $Y$ 之间的二值交叉熵损失（BCE损失）：

\begin{equation}
l_i = \text{LBCE}(M_i, Y) \quad (i = 1, 2, 3, 4)
\end{equation}

其中，$\text{LBCE}(M_i, Y)$ 是第 $i$ 个尺度的二值交叉熵损失，定义如下：

\begin{equation}
\text{LBCE}(M_i, Y) = -\frac{1}{N} \sum_{k=1}^{N} \left[ Y_k \log(M_{i,k}) + (1 - Y_k) \log(1 - M_{i,k}) \right]
\end{equation}

其中，$N$ 是样本数量，$Y_k$ 是样本 $k$ 的真实标签，$M_{i,k}$ 是第 $i$ 个尺度的预测值。

最终，BDFU-Net的总损失由五项损失组成：$l_1$、$l_2$、$l_3$、$l_4$ 和 $l_\Sigma$，其中 $l_\Sigma$ 是所有显著性图的合并损失，定义为：

\begin{equation}
l_\Sigma = \text{LBCE}(M_\Sigma, Y)
\end{equation}

$M_\Sigma$ 是通过对所有多尺度显著性图进行拼接和上采样得到的最终显著性图，公式如下：

\begin{equation}
M_\Sigma = \sigma\left(f_{1\times1} [M_1, B(M_2), B(M_3), B(M_4)]\right)
\end{equation}

其中，$B(\cdot)$ 表示对显著性图进行双线性插值（Bilinear Interpolation）。

最终的总损失函数为：

\begin{equation}
L = l_1 + l_2 + l_3 + l_4 + l_\Sigma
\end{equation}

通过深监督结构与多尺度损失的结合，BDFU-Net能够在多个层级上同时进行优化，进一步提高了对小目标的检测精度，并加速了网络的收敛。

\subsection{深监督的作用}

深监督结构的引入对BDFU-Net的训练具有重要作用。首先，深监督通过在网络的多个层次进行监督，提供了多尺度的信息，使得网络能够从不同的尺度上同时学习目标的特征，避免了单一尺度下目标信息的丢失。其次，深监督机制加速了网络的收敛过程，使得BDFU-Net能够更快地学习到小目标的精细特征，从而提高了模型的训练效率。

此外，深监督结构使得网络能够在每个尺度上进行细粒度的优化，进一步增强了网络在复杂背景下的鲁棒性。这对于红外小目标检测任务尤其重要，因为小目标通常具有较低的对比度，深监督能够帮助网络更好地学习到目标的细节特征，并提高目标与背景的区分度。


\section{实验}

\subsection{数据集与评估指标}

我们使用公开的NUAA-SIRST\cite{}、NUDT-SIRST\cite{}和IRSTD-1K\cite{}数据集进行实验。NUAA-SIRST和NUDT-SIRST的训练和测试集使用\cite{}中的方法进行划分，而IRSTD-1K的数据集则按照\cite{}中的方法进行划分。

我们使用以下指标评估我们方法的性能：IoU、标准化IoU（nIoU）\cite{}、检测概率（Pd）、误报率（Fa）、F1分数、接收者操作特征（ROC）曲线以及曲线下面积（AUC），其中IoU、nIoU、Pd、Fa和F1的固定阈值设为0.5。



\textbf{IoU} 是像素级的评估指标，定义为：
\begin{equation}
\text{IoU} = \frac{A_{\text{inter}}}{A_{\text{union}}}
\end{equation}
其中，$A_{\text{inter}}$ 和 $A_{\text{union}}$ 分别表示交集区域和并集区域。

\textbf{nIoU} 定义为：
\begin{equation}
\text{nIoU} = \frac{1}{N} \sum_{i=1}^{N} \frac{A_{\text{inter}}}{A_{\text{union}}}
\end{equation}
其中，$N$ 是测试集中的样本数量，$i$ 是第$i$个样本。

\textbf{Pd} 是正确预测目标的比率：
\begin{equation}
\text{Pd} = \frac{N_{\text{pred}}}{N_{\text{all}}}
\end{equation}
其中，$N_{\text{pred}}$ 是正确预测的目标数，$N_{\text{all}}$ 是总目标数。按照\cite{}的定义，如果目标质心的偏差小于3，则视为正确预测。

\textbf{Fa} 是误报像素的比率：
\begin{equation}
\text{Fa} = \frac{N_{\text{false}}}{P_{\text{all}}}
\end{equation}
其中，$N_{\text{false}}$ 是误预测的目标像素数，$P_{\text{all}}$ 是图像中的所有像素。

\textbf{F1分数} 用来评估像素级的漏检与误报情况，其中精确度（Precision）和召回率（Recall）分别表示为：
\begin{equation}
F1 = \frac{2 \times \text{Precision} \times \text{Recall}}{\text{Precision} + \text{Recall}}
\end{equation}

除了固定阈值的评 \cite{}估方法外，我们还利用接收者操作特征（ROC）曲线来全面评估模型的表现。ROC曲线用来描述在不同误报率下，检测概率（Pd）的变化趋势。








\subsection{实验设置}


\subparagraph{细节}

我们使用包含四个残差块（RBs）的 U-Net 作为检测骨干网络，网络包含四个下采样层，基础通道宽度设为 32。所有输入图像都会被归一化，并随机裁剪为 $256 \times 256$ 的尺寸。为了防止过拟合，我们通过随机翻转与旋转等方式对训练数据进行增强。模型的权重和偏置采用 Kaiming 初始化方法\cite{}。训练过程使用 BCE 损失函数，并由 Adam 优化器进行优化。初始学习率设为 0.001，并采用余弦退火策略逐步降低至 $1 \times 10^{-5}$。训练总轮数设为 1000。按照\cite{} 的设置，显著图的分割阈值固定为 0.5。

\subparagraph{基准模型选择}

为了评估我们模型的性能并与已有基准保持一致，我们在 NUAA-SIRST、NUDT-SIRST 和 IRSTD-1K 数据集上，与六种基于深度学习的方法进行了比较，包括 ACM\cite{}、ALCNet\cite{}、RDIAN\cite{}、ISTDU-Net\cite{}、DNANet\cite{} 和 UIU-Net\cite{}。为确保公平性，所有方法均基于相同的数据集重新训练。


\subsection{量化结果}

表格 \ref{tab:nuaa}、\ref{tab:nudt} 和 \ref{tab:irstd-1k} 展示了各模型性能的对比分析。在尽可能保证 IoU 和 nIoU 指标较高的同时，$P_d$ 与 $F_a$ 之间仍然存在权衡关系。我们的模型更加注重在 $P_d$ 与 $F_a$ 之间取得整体平衡。综合指标 $F_1$ 进一步表明，通过多层次特征融合，我们的模型能够更加有效地理解与表达语义信息。

在 NUAA-SIRST 数据集上，我们的模型在 IoU、nIoU、$P_d$ 和 $F_1$ 指标上均取得了最优表现，而 $F_a$ 指标仅次于 UIUNet。对于 NUDT-SIRST 数据集，虽然 $P_d$ 和 $F_a$ 并非最优，但我们的模型在 IoU 和 nIoU 上均排名第二，并保持了较高的 $F_1$ 得分。整体指标略逊于 DNANet 的原因在于 \cite{} 中采用的 1:1 数据集划分方式，使得我们的模型无法充分学习该数据集中多样化的特征。即使面对高度挑战性的 IRSTD-1k 数据集，我们的模型仍然在 IoU 与 nIoU 指标上取得最佳表现。尽管在 $P_d$ 方面未达到最优，但其在降低 $F_a$ 上表现显著，这在高风险、实时性强以及资源受限的应用场景中尤为关键。

表格 \ref{tab:model_comparison} 给出了三类数据集上不同算法复杂度的平均测量结果。可以观察到，BDF U-Net 的整体性能最高，且优于性能强大的 UIU-Net 和 SCTransNet。

仅基于固定阈值评估性能能够提供的洞察有限。因此，按照 IRSTD 相关研究 \cite{} \cite{} \cite{} 的常用做法，我们使用 ROC 曲线与 AUC 指标来衡量模型在不同阈值下的判别能力。Fig. \ref{fig:roc} 展示了我们的方法在 ROC 曲线上优于其他方法，而 Table \ref{tab:auc} 则显示我们的方法在各项方法中取得了最高的 AUC 分数。



% nuaa中的性能比较
\begin{table}[ht]
\centering
\caption{在 NUAA-SIRST 数据集，我们的方法和其他深度学习方法的比较结果 $IoU \uparrow$(\%), $nIoU \uparrow$(\%), $P_d \uparrow$(\%), $F_a \downarrow$(10\textsuperscript{-6}).}
\label{tab:nuaa}
\begin{tabular}{lcccc}
\toprule
\textbf{Method} & IoU & nIoU & $P_d$ & $F_a$ \\
\midrule
ALCNet\cite{} & 60.26 & 58.41 & 86.69 & 57.41 \\
ACM\cite{} & 68.33 & 67.46 & 91.63 & 24.97 \\
ISNet \cite{} & 73.38 & 70.49 & 95.05 & 67.98 \\
RDIAN\cite{} & 69.61 & 73.59 & 94.67 & 50.21 \\
ISTDU-Net\cite{} & 74.72 & 77.66 & \underline{95.81} & 40.88 \\
DNANet \cite{} & 73.59 & 79.19 & 93.15 & 40.26 \\
UIUNet \cite{} & 76.20 & 79.21 & 92.015 & \textbf{11.45} \\
SCTransNet \cite{} & \underline{76.52} & \underline{80.03} & 95.75 & 16.86 \\
Ours & \textbf{77.35} & \textbf{81.00} & \textbf{95.81} & \underline{15.92} \\
\bottomrule
\end{tabular}
\end{table}





% nudt中性能比较
\begin{table}[ht]
\centering
\caption{在 NUDT-SIRST 数据集，我们的方法和其他深度学习方法的比较结果  $IoU \uparrow$(\%), $nIoU \uparrow$(\%), $P_d \uparrow$(\%), $F_a \downarrow$(10\textsuperscript{-6}).}
\label{tab:nudt}
\begin{tabular}{lcccc}
\toprule
\textbf{Method} & IoU & nIoU & $P_d$ & $F_a$ \\
\midrule
ALCNet\cite{} & 61.12 & 60.59 & 97.24 & 29.09 \\
ACM\cite{} & 64.85 & 66.89 & 96.71 & 28.58 \\
ISNet \cite{} & 82.51 & 81.23 & 97.77 & 6.34 \\
RDIAN\cite{} & 82.41 & 83.69 & \underline{98.84} & 14.84 \\
ISTDU-Net\cite{} & 91.76 & 92.22 & 98.18 & \underline{3.76} \\
DNANet \cite{} & \textbf{94.19} & \textbf{94.44} & \textbf{99.25} & \textbf{2.43} \\
UIUNet \cite{} & 90.51 & 90.58 & 98.83 & 8.34 \\
SCTransNet \cite{} & 92.09 & 92.38 & 98.83 & 4.29 \\
Ours & \underline{93.30} & \underline{93.90} & 98.73 & 5.15 \\
\bottomrule
\end{tabular}
\end{table}



% irstd-1k中的性能比较
\begin{table}[ht]
\centering
\caption{在 IRSTD-1K 数据集，我们的方法和其他深度学习方法的比较结果 $IoU \uparrow$(\%), $nIoU \uparrow$(\%), $P_d \uparrow$(\%), $F_a \downarrow$(10\textsuperscript{-6}).}
\label{tab:irstd-1k}
\begin{tabular}{lcccc}
\toprule
\textbf{Method} & IoU & nIoU & $P_d$ & $F_a$ \\
\midrule
ALCNet\cite{} & 58.08 & 58.72 & 92.92 & 74.49 \\
ACM\cite{} & 60.32 & 57.21 & \underline{93.26} & 68.49 \\
ISNet \cite{} & 62.22 & 61.85 & 90.23 & 31.56 \\
RDIAN\cite{} & 59.93 & 61.04 & 87.20 & 33.34 \\
ISTDU-Net\cite{} & 65.01 & 64.83 & \textbf{93.93} & 26.43 \\
DNANet \cite{} & 65.73 & 66.43 & 89.46 & 12.33 \\
UIUNet \cite{} & 65.69 & 65.94 & 91.24 & 13.47 \\
SCTransNet \cite{} & \underline{67.03} & \textbf{68.15} & 93.24 & \underline{11.74} \\
Ours & \textbf{67.66} & \underline{68.11} & 93.25 & \textbf{9.72} \\
\bottomrule
\end{tabular}
\end{table}

% 参数量和浮点数比较
\begin{table}[ht]
\centering
\caption{Model comparison in terms of parameters, computational complexity, and performance metrics.}
\label{tab:model_comparison}
\begin{tabular}{lccccc}
\hline
Model & Params (M) & Flops (G) & IoU & nIoU & $F_1$ \\ 
\hline
DNA-Net \cite{} & 4.697 & 14.26 & 80.23 & 82.59 & 88.60 \\
UIU-Net \cite{} & 50.54 & 54.42 & 82.40 & 86.12 & 90.35 \\
SCTransNet \cite{}& 11.19 & 20.24 & 83.03 & 86.66 & 90.89 \\
Ours & 9.89 & 10.12 & 83.12 & 86.79 & 90.91 \\

\hline
\end{tabular}
\end{table}


% auc指标比较
\begin{table}[htbp]
\centering
\caption{AUC Comparison}
\small % 使用小字体使表格适合单列
\begin{tabular}{lccc}
\toprule
\textbf{Method} & \textbf{NUAA-SIRST} & \textbf{NUDT-SIRST} & \textbf{IRSTD-1K} \\
     & \textbf{ AUC} & \textbf{AUC} & \textbf{AUC} \\
\midrule
ACM & 0.9375 & 0.9428 & \underline{0.9332} \\
ALCNet & \underline{0.9462} & \underline{0.9870} & 0.9019 \\
DNANet & 0.9166 & 0.9832 & 0.8883 \\
UIUNet & 0.9102 & 0.9477 & 0.7901 \\
RDIAN & 0.9246 & 0.9470 & 0.8717 \\
ISTDU-Net & 0.9450 & 0.9793 & 0.9110 \\
Ours & \textbf{0.9679} & \textbf{0.9916} & \textbf{0.9493} \\
\bottomrule
\end{tabular}
\label{tab:auc}
\end{table}



\begin{figure}[htbp]
	\centering
	\begin{subfigure}[b]{.45\textwidth}  % 注意此处的尺寸控制
		\centering
		\includegraphics[width = \textwidth]{roc/roc-nuaa.png}
		\caption{不同方法在NUAA上的ROC曲线}\label{fig:subfig-samp1}
	\end{subfigure}
	\begin{subfigure}[b]{.45\textwidth}
		\centering
		\includegraphics[width = \textwidth]{roc/roc-nudt.png}
		\caption{不同方法在NUDT上的ROC曲线}\label{fig:subfig-samp2}
	\end{subfigure}
	\begin{subfigure}[b]{.45\textwidth}
		\centering
		\includegraphics[width = \textwidth]{roc/roc-irstd1k.png}
		\caption{不同方法在IRSTD-1K上的ROC曲线}\label{fig:subfig-samp3}
	\end{subfigure}
	\caption{不同深度学习方法在三个数据集上的ROC曲线}\label{fig:roc}
\end{figure}



\subsection{可视化结果}
Table \ref{tab:preds} 展示了六种具有代表性的算法在 NUAA、NUDT 和 IRSTD-1k 数据集上的预测结果。与其他方法相比，我们的方法不仅能够有效降低漏检和误检数量，还展现出更强的目标轮廓预测能力。

如 Table \ref{tab:preds}(a) 所示，我们的方法成功识别了两个距离非常接近的目标，而其他深度学习方法则错误地将它们融合为一个目标。
在 Table \ref{tab:preds}(c) 和 (e) 中，我们的方法能够有效避免误检，并且更准确地预测目标轮廓，其预测结果与真实标注的轮廓更加一致。
在 Table \ref{tab:preds}(b) 和 (d) 中，性能强大的 DNANet\cite{} 和 UIUNet\cite{} 仍然出现了漏检情况，而我们的方法能够清晰地区分复杂背景与目标。
在 Table \ref{tab:preds}(f) 中展示了预测失败的案例。当背景与目标在外观上非常相似时，我们的方法与其他方法均会产生误检。


% 预测图展示


\begin{table}[t]
\centering
\resizebox{\textwidth}{!}{
\begin{tabular}{c|c|c|c|c|c|c|c|c}
\toprule
\textbf{ID} & \textbf{Image} & \textbf{RDIAN} & \textbf{ACM} &
\textbf{ISTDU-Net} & \textbf{DNANet} & \textbf{UIUNet} &
\textbf{Ours} & \textbf{GT} \\
\midrule

(a) &
\includegraphics[width=0.12\textwidth]{visual/misc59/im.png} &
\includegraphics[width=0.12\textwidth]{visual/misc59/rdian.png} &
\includegraphics[width=0.12\textwidth]{visual/misc59/acm.png} &
\includegraphics[width=0.12\textwidth]{visual/misc59/istdu.png} &
\includegraphics[width=0.12\textwidth]{visual/misc59/dna.png} &
\includegraphics[width=0.12\textwidth]{visual/misc59/uiu.png} &
\includegraphics[width=0.12\textwidth]{visual/misc59/ours.png} &
\includegraphics[width=0.12\textwidth]{visual/misc59/gt.png}
\\

(b) &
\includegraphics[width=0.12\textwidth]{visual/misc379/im.png} &
\includegraphics[width=0.12\textwidth]{visual/misc379/rdian.png} &
\includegraphics[width=0.12\textwidth]{visual/misc379/acm.png} &
\includegraphics[width=0.12\textwidth]{visual/misc379/istdu.png} &
\includegraphics[width=0.12\textwidth]{visual/misc379/dna.png} &
\includegraphics[width=0.12\textwidth]{visual/misc379/uiu.png} &
\includegraphics[width=0.12\textwidth]{visual/misc379/ours.png} &
\includegraphics[width=0.12\textwidth]{visual/misc379/gt.png}
\\
\midrule

(c) &
\includegraphics[width=0.12\textwidth]{visual/00906/im.png} &
\includegraphics[width=0.12\textwidth]{visual/00906/rdian.png} &
\includegraphics[width=0.12\textwidth]{visual/00906/acm.png} &
\includegraphics[width=0.12\textwidth]{visual/00906/istdu.png} &
\includegraphics[width=0.12\textwidth]{visual/00906/dna.png} &
\includegraphics[width=0.12\textwidth]{visual/00906/uiu.png} &
\includegraphics[width=0.12\textwidth]{visual/00906/ours.png} &
\includegraphics[width=0.12\textwidth]{visual/00906/gt.png}
\\

(d) &
\includegraphics[width=0.12\textwidth]{visual/00542/im.png} &
\includegraphics[width=0.12\textwidth]{visual/00542/rdian.png} &
\includegraphics[width=0.12\textwidth]{visual/00542/acm.png} &
\includegraphics[width=0.12\textwidth]{visual/00542/istdu.png} &
\includegraphics[width=0.12\textwidth]{visual/00542/dna.png} &
\includegraphics[width=0.12\textwidth]{visual/00542/uiu.png} &
\includegraphics[width=0.12\textwidth]{visual/00542/ours.png} &
\includegraphics[width=0.12\textwidth]{visual/00542/gt.png}
\\
\midrule

(e) &
\includegraphics[width=0.12\textwidth]{visual/xdu968/im.png} &
\includegraphics[width=0.12\textwidth]{visual/xdu968/rdian.png} &
\includegraphics[width=0.12\textwidth]{visual/xdu968/acm.png} &
\includegraphics[width=0.12\textwidth]{visual/xdu968/istdu.png} &
\includegraphics[width=0.12\textwidth]{visual/xdu968/dna.png} &
\includegraphics[width=0.12\textwidth]{visual/xdu968/uiu.png} &
\includegraphics[width=0.12\textwidth]{visual/xdu968/ours.png} &
\includegraphics[width=0.12\textwidth]{visual/xdu968/gt.png}
\\

(f) &
\includegraphics[width=0.12\textwidth]{visual/xdu673/im.png} &
\includegraphics[width=0.12\textwidth]{visual/xdu673/rdian.png} &
\includegraphics[width=0.12\textwidth]{visual/xdu673/acm.png} &
\includegraphics[width=0.12\textwidth]{visual/xdu673/istdu.png} &
\includegraphics[width=0.12\textwidth]{visual/xdu673/dna.png} &
\includegraphics[width=0.12\textwidth]{visual/xdu673/uiu.png} &
\includegraphics[width=0.12\textwidth]{visual/xdu673/ours.png} &
\includegraphics[width=0.12\textwidth]{visual/xdu673/gt.png}
\\

\bottomrule
\end{tabular}}
\caption{Visualization results.}
\label{tab:preds}
\end{table}



\subsection{消融实验}

在本节中，我们通过消融实验验证了 BDF U-Net 的有效性。

首先，我们将残差块（Residual Blocks, RBs）、深度监督（Deep Supervision, DS）、CCA 模块以及 BDF 模块逐步加入到基础 U-Net 中，以验证这些模块在 IRSTD 任务中的有效性。其结果如 Table \ref{table:ab1} 所示。我们可以观察到，随着这些模块的不断添加，算法的性能持续提升。特别是 BDF 模块，显著增强了算法在 IoU、nIoU 和 $F_1$ 指标上的表现。

此外，我们测试了模块内部结构设计的有效性。我们将 DFF 或 UFF 替换为原始跳跃连接，以探索 DFF 与 UFF 在特征融合过程中的贡献。实验结果如 Table \ref{table:ab2} 所示，表明 UFF 相较于 DFF 对性能提升的贡献更大。这是由于原始 U-Net 的编码器结构倾向于将自底向上传递的融合信息逐层上传。值得注意的是，尽管 DFF 单独作用时的效果较为有限，但实验数据表明，将其与 UFF 结合使用能够进一步提升模型性能。

在 DFF 模块中，我们采用了重新组织的下采样方式，以替代传统的池化操作。Table \ref{table:ab3} 的对比结果表明，该重新组织的下采样方法优于最大池化（maxpooling）下采样。

最后，Table \ref{table:ab4} 展示了关于 BDF 数量与模型基础宽度的超参数研究结果。我们观察到，当 \(N = 2\) 时，模型性能达到最佳。然而，当 \(N\) 超过 2 时，性能开始下降，这表明过度的特征融合可能会导致错误信息的累积。因此，在本文提出的 BDF U-Net 中，BDF 的数量与模型的基础宽度分别设置为 2 和 32。






% 外部模块消融
\begin{table}[htbp]
\centering
\caption{Based on U-Net, ablation study of the Residual Blocks (RBs), Deep Supervision (DS), CCA and Bidirectional Fusion Block (BDF) in IoU(\%), nIoU(\%), $F_1$(\%) on NUAA-SIRST, NUDT-SIRST and IRSTD-1K.}


\begin{tabular}{cccccc|ccc}
\toprule
\textbf{U-Net} & \textbf{+RBs} & \textbf{+DS} & \textbf{+CCA}  & \textbf{+BDF} & & \textbf{IoU} & \textbf{nIoU} & \textbf{$F_1$} \\
\midrule
\checkmark &  &  &  &  && 75.29 & 78.60 & 86.36 \\ 
\checkmark & \checkmark &  &  &  && 77.07 & 80.13 & 87.05 \\ 
\checkmark & \checkmark & \checkmark &  &   && 77.73 & 80.78 & 87.47 \\ 
\checkmark & \checkmark & \checkmark & \checkmark &  && 78.02 & 80.45 & 87.66 \\ 
\checkmark & \checkmark & \checkmark & \checkmark & \checkmark  && \textbf{83.12} & \textbf{86.79} & \textbf{90.91} \\ 
\bottomrule
\end{tabular}
\label{table:ab1}
\end{table}


\begin{table}[htbp]
\centering
\caption{Comparison of fusion methods in IoU(\%), nIoU(\%), and $F_1$(\%).}
\label{table:fusion_methods}
\begin{tabular}{lccc}
\toprule
\textbf{Method} & \textbf{IoU} & \textbf{nIoU} & \textbf{$F_1$} \\ 
\midrule
No fusion & 78.02 & 80.45 & 87.66 \\ 
Only DFF & 79.31 & 79.60 & 88.56 \\ 
Only UFF  & 81.27 & 84.14 & 89.27 \\ 
\textbf{DFF+UFF} & \textbf{83.12} & \textbf{86.79} & \textbf{90.91} \\ 
\bottomrule
\end{tabular}
\label{table:ab2}
\end{table}


\begin{table}[htbp]
\centering
\caption{Comparison of downsampling methods in DFF in IoU(\%), nIoU(\%), and $F_1$(\%)}
\label{table:method_comparison}
\begin{tabular}{lccc}
\toprule
\textbf{Method} & \textbf{IoU} & \textbf{nIoU} & \textbf{$F_1$} \\ 
\midrule
Maxpool & 81.58 & 85.56 & 89.85 \\ 
\textbf{Reorganize} & \textbf{83.12} & \textbf{86.79} & \textbf{90.91} \\ 
\bottomrule
\end{tabular}
\label{table:ab3}
\end{table}

% 超参数选择
\begin{table}[htbp]
\centering
\caption{Hyper-parameter study of the number of Fusion Blocks and the basic width of the model in terms of IoU(\%), nIoU(\%), and $F_1$(\%).}
\label{table:fusion_blocks}
\begin{tabular}{cccc}
\toprule
\textbf{Hyper-param} & \textbf{IoU} & \textbf{nIoU} & \textbf{$F_1$} \\ 
\midrule
\multicolumn{4}{c}{\textbf{The number of Fusion Blocks}} \\ 
$N=1$ & 82.12 & 86.11 & 90.18 \\ 
$N=2$ & \textbf{83.12} & \textbf{86.79} & \textbf{90.91} \\ 
$N=3$ & 81.76 & 85.41 & 89.96 \\ 
\midrule
\multicolumn{4}{c}{\textbf{The basic width of the model}} \\ 
$W=16$ & 80.38 & 84.14 & 89.12 \\ 
$W=32$ & \textbf{83.12} & \textbf{86.79} & \textbf{90.91} \\ 
$W=48$ & 80.01 & 83.53 & 88.65 \\ 
\bottomrule
\end{tabular}
\label{table:ab4}
\end{table}






通过阅读上一章，
相信您已基本配置完成编辑环境，
以及如何正确编写各级章节和段落，
了解容易引起编译出错的逃逸字符。

如果您仔细阅读过源码，
您应该已经懂得使用命令\texttt{$\backslash$begin\{xxxx\}}开始一段新的布局环境。
现在将稍系统地介绍计算机类学位论文中的排版元素，及其编写方法。
本章主要介绍以下排版元素：
\begin{itemize}
    \item 列表环境（包括有序、无序、定义三种列表）
    \item 插图和表格
    \item 代码环境
    \item 数学和算法环境
\end{itemize}

本章尽量覆盖论文写作中的大部分场景，但不面面俱到。
如有特殊需求，请仔细阅读相关宏包手册或求助于国内外TeX社区及问答网站。


\section{插图环境和浮动体}

相信您在上一章的探索学习中已经基本掌握了插入图片的方法，
但可能仍存疑虑。
现在先简单介绍浮动体的概念，
以助您理解插图环境的布局规则，
最后再介绍子图的排布以应对您更高的排版需求。
% 关于绘图，本文将在后续章节讲述
关于图的绘制，本文将在\ref{how-to-plot} 继续讲述。 % 活用ref引用，让评阅老师随处移动

当一个图片或表格太大在当前页面无法继续排版时，
一种简单的解决方案，
即是新开一页排版（Word 默认模式），
前页可能留下大段空白，十分不美观。
\LaTeX 的默认解决方案是把它们“浮动”到下一页，
与此同时将后续正文文本填充到插入点后。

插图和表格在\LaTeX 排版中默认为一个浮动体，
当排版引擎试图放置一个浮动体时，它将遵循以下规则：
\begin{enumerate}
    \item 浮动体的布局大小不得超过版心\footnote{版心是指排版文字和图表的区域，一般在页面的中心。——百度百科}，否则不能通过编译(Overfull Page Error)
    \item 浮动体只能向后浮动，无法向前浮动
    \item 浮动体默认按照 h $\to$ t $\to$ b $\to$ p 规则布局
    \begin{description}
        \item[h] 排布在当前位置，如果本页所剩空间不够，忽略，检查规则 t
        \item[t] 浮动到下一页顶部
        \item[b] 浮动到下一页底部（脚注之下）
        \item[p] 浮动到一个允许出现浮动体的页面
        \item[!] 忽略浮动体放置的大多数内部参数\footnote{在下也不太懂}
    \end{description}
    \item 设置 htbp 参数的顺序不会影响默认的规则顺序
\end{enumerate}
在实践中，一般选用浮动规则[htbp], [tbp], [htp], [tp] 来完成浮动体布局。
请不要使用单一参数布局，这样极有可能出现难解的浮动问题。
不适当的浮动规则参数将导致浮动对象被放进一个队列中等待布局，
如果队列中浮动对象超过 18 个，编译时报Too Many Unprocessed Floats错误。
当需要在一页中排版的图片较多时，
您可以通过\texttt{$\backslash$clearpage}命令强制在此处排版完所有浮动体
后在排版其他内容。关于清除浮动等复杂主题，此处不再展开。

一般实践中，插图尺寸不宜超过版心一半，插图也不宜过密。
另外，可以在论文内容稳定后，
通过前置插图代码，
强行“向前浮动”，保证插图和引用处的距离不至于太远。
% 破坏语义，不宜滥用

关于本模板对浮动体的设置，参看\texttt{zjuthesis.cls}，
搜索关键字“浮动体”找到对应配置。
图片引用路径在\texttt{zjuthesis.cls}里定义的\texttt{graphicspath}里，
默认情况下，\texttt{$\backslash$includegraphics}命令从论文源码根目录搜索，
如果在根目录里找到文件，则不再继续往定义引用路径搜索，
当引擎无法找到您指定的图片资源时，会导致编译错误。
注意，引用的文件名包括文件后缀。

% 现在你可以随意更动此插图代码的位置来感受一下浮动体布局的规则


接下来描述子图的编写，
在实际论文撰写过程中，
经常需要比较几组实验数据或场景。
此时，合乎语义的做法是为不同的组设置子图，
而不是分别设图。

多个子图组成一个单独的浮动体布局，
共用一个总图题和总引用，并可以有各自单独的子图题和引用。
本模板使用subcaption 宏包处理子图排版，如\autoref{fig:subfig-samp} 所示
论文中不可像本文一般，
平白无故地出现与行文毫无关联的图例，
而且，必须有适当的文字内容对图例做出解释。
比如，比较分析从\autoref{fig:subfig-samp1} 到\autoref{fig:subfig-samp3}
仙剑系列在白学梗方面的运用变迁。\footnote{往后数代仍有类似场景 -\_-\# （顔文字書込禁止!）}

当准备插图资源时，应该尽可能保证插图清晰，背景透明。
图中文字大小应与文中接近，不小于脚注文字大小，不大于正文段落文字大小，
框线宽度不大于2px。

如果您曾关注过图片的格式，
应该知道图片在计算机中一般分为矢量图（\autoref{fig:vector}）和位图（\autoref{fig:raster}）两种类型。
通俗地说，矢量图通过几何属性存储图片信息，
所以能在缩放时保持图形的几何属性。
而位图按像素点存储图片信息，在缩放时必然会丢失信息。
对于学位论文里的图例，请尽量使用矢量图，
以给评阅老师或后人精确地参考和还原实验。
常用的矢量图格式有eps, pdf, svg 和 Adobe 系列的文件格式。
其中\LaTeX 格式可以直接引用eps 和 pdf 格式的图片。

\begin{figure}[htbp]
	\centering
	\begin{subfigure}[b]{.45\textwidth}
		\centering
		\includegraphics[width = \textwidth]{vector.pdf}
		\caption{矢量图}\label{fig:vector}
	\end{subfigure}
	\begin{subfigure}[b]{.45\textwidth}
		\centering
		\includegraphics[width = \textwidth]{raster.png}
		\caption{位图}\label{fig:raster}
	\end{subfigure}
	\caption{Google Logo 的矢量图和位图比较}\label{fig:vector-raster}
\end{figure}




\begin{algorithm}
\DontPrintSemicolon
\KwIn{A sequence of integers $\langle a_1, a_2, \ldots, a_n \rangle$}
\KwOut{The index of first location with the same value as in a previous location in the sequence}
$location \gets 0$\;
$i \gets 2$\;
\While{$i \leq n \land location = 0$} {
  $j \gets 1$\;
  \While{$j < i \land location = 0$} {
    % The "l" before the If makes it so it does not expand to a second line
    \lIf{$a_i = a_j$} {
      $location \gets i$\;
    }
    \lElse{
      $j \gets j + 1$\;
    }
  }
  $i \gets i + 1$\;
}
\Return{location}\;
\caption{{\sc FindDuplicate2}}
\label{algo:duplicate2}
\end{algorithm}


\section{关于参考文献}

硕士学位论文的参考文献，
请严格按照导师和学院规定，
注重引文质量，万不可滥引。

参考文献参照国家标准《GB/T 7714-2005: 文后参考文献著录规则》
\footnote{此标准规定的学位论文引用格式并无指定需列出是“硕士学位论文”还是“博士学位论文”}，
样式文件由南京大学胡海星提供。
\begin{verbatim}
http://haixing-hu.github.io/nju-thesis/
\end{verbatim}

学校规定，参考文献采用顺序编码制，
即引文处采用序号标注，参考文献表按引文序号顺序列出。
参考文献的排版需要引入同学们自己的文献数据库，
南京大学胡海星提供了一个样例数据库，见其代码仓库内\texttt{references/test.bib}。
通过各式文献管理工具（如Zotero)，您可以在论文早期工作时逐渐积累文献数据库。
通过Google学术查找一篇文献时，如\autoref{fig:gscholar} 所示，点击cite，
选择BibTeX，即可得到本文献的Bib格式的各项字段。
\begin{figure}[htbp]
    \centering
    \includegraphics[width=\textwidth]{gscholar.png}
    \caption{使用Google学术查找引文的BibTeX字段}
    \label{fig:gscholar}
\end{figure}

由Google学术提供的文献类型和字段有可能不满足胡海星前辈的设定，
注意调整。
以下是常用的文献类型：
\begin{description}
    \item[期刊]          \texttt{@article}
    \item[专著]          \texttt{@book, @inbook}
    \item[译著]          \texttt{@Book, @inBook}
    \item[会议论文集]    \texttt{@proceeding, @inproceeding}
    \item[手册]          \texttt{@manual}
    \item[网页]          \texttt{@webpage, @online}
\end{description}

\begin{itemize}
    \item 比如这是一篇中文期刊\cite{lixiaodong1999}
    \item 这是几篇英文期刊\cite{christine1998, kanamori1998}
    \item 一本中文书\cite{zh-book-1}
    \item 一本中文译著\cite{anwen1988b}
    \item 一本英文书\cite{lamport1994latex, takeuti1973}
    \item 一篇中文inproceeding\cite{nonlinear1996}
    \item 中文proceeding\cite{a2-1}
    \item 英文proceeding\cite{a2-2}
    \item 中文inproceeding\cite{aczel1998}
    \item 一篇学位论文\cite{a4-1} 
    \item 其他资料：手册\cite{ipad}报纸\cite{renminribao}网页\cite{dubash2010}
\end{itemize}

在论文中设置了一个错误或丢失的引用不会引起编译错误，
引擎会在引用标记中设一个问号。
手动编译论文的顺序一般为：
\begin{verbatim}
xelatex main
bibtex main  // 生成参考文献
xelatex main
xelatex main
\end{verbatim}
而latexmk 自动化地执行了这些步骤，所以编译时间才需要20余秒之久。







\section{本章小结}

本章划分节比较多，正式行文中请尽量避免。

传播智识，单单借助文字的力量是无力的，
即使是日常博客文章，列表、插图、表格、代码都少不了。
何况是一篇用于申请硕士学位的论文呢？

一篇学位论文集长期的科研工程实践智慧于寥寥数万字。
如何合理规划论文语义和排版元素，
让即便不熟习此领域的后人能在短时间内消化，
得以继续开物前民，
是一个值得反复求索的话题。

